---
title: "trips_per_day"
author: "Tenzin Chosang"
date: "2023-06-20"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## RMarkdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:


## Setup

First we'll load some packages, including the recent modelr for easy modeling, setting options to warn us whenever observations with missing values are ignored by our models.

```{r}
library(tidyverse)
library(scales)
library(modelr)

theme_set(theme_bw())
options(repr.plot.width=4, repr.plot.height=3)
```

Then we'll load a data frame of the number of total trips taken by Citibike riders for each day in 2014, along with the weather on each day.

```{r}
trips_per_day <- read_tsv('trips_per_day.tsv')
head(trips_per_day)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.

Let's plot the number of trips taken as a function of the minimum temperature on each day.

```{r}
ggplot(trips_per_day, aes(x = tmin, y = num_trips)) +
  geom_point() +
  xlab('Minimum temperature') +
  ylab('Daily trips') +
  scale_y_continuous()
```

## Cross-validation

Now we'll try fitting different polynomials to this data, and use cross-validation to find the polynomial degree that generalizes best to held out data.

First we'll shuffle the data and make an 80% train and 20% validation split.

```{r}
set.seed(42)

num_days <- nrow(trips_per_day)
frac_train <- 0.9
num_train <- floor(num_days * frac_train)

# randomly sample rows for the training set 
ndx <- sample(1:num_days, num_train, replace=F)

# used to fit the model
trips_per_day_train <- trips_per_day[ndx, ]

# used to evaluate the fit
trips_per_day_validate <- trips_per_day[-ndx, ]
```

set.seed(42)

num_days <- nrow(trips_per_day)
frac_train <- 0.8
num_train <- floor(num_days * frac_train)

# randomly sample rows for the training set 
ndx <- sample(1:num_days, num_train, replace=F)

# used to fit the model
trips_per_day_train <- trips_per_day[ndx, ]

# used to evaluate the fit (validation and test set)
trips_per_day_validate_and_test <- trips_per_day[-ndx, ]

# split the validation set and test set
num_validate <- floor(nrow(trips_per_day)*((1-frac_train)/2))
num_validate

ndx_v <-sample(trips_per_day_validate_and_test, num_validate, replace=F)
ndx_v
ndx
# validation set for validating models




# test set that we don't touch until the very end





# FROM STACKOVERFLOW - DOES NOT CUT PERFECTLY 80-10-10
idx <- sample(size = nrow(trips_per_day), replace = TRUE, prob = c(.8, .1, .1))
trips_per_day_train <- trips_per_day[idx == 1,]
trips_per_day_validate <- trips_per_day[idx == 2,]
trips_per_day_test <- trips_per_day[idx == 3,]





Now we'll evaluate models from degree 1 up through degree 8. For each we'll fit on the training data and evaluate on the validation data.

```{r}
# fit a model for each polynomial degree
K <- 1:8
train_err <- c()
validate_err <- c()
for (k in K) {
  
    # fit on the training data
    model <- lm(num_trips ~ poly(tmin, k, raw = T), data=trips_per_day_train)
    
    # evaluate on the training data
    train_err[k] <- sqrt(mean((predict(model, trips_per_day_train) - trips_per_day_train$num_trips)^2))

    # evaluate on the validate data
    validate_err[k] <- sqrt(mean((predict(model, trips_per_day_validate) - trips_per_day_validate$num_trips)^2))
}
```

Now we'll plot the training and validation error as a function of the polynomial degree.

```{r}
plot_data <- data.frame(K, train_err, validate_err) %>%
  gather("split", "error", -K)

ggplot(plot_data, aes(x=K, y=error, color=split)) +
  geom_line() +
  scale_x_continuous(breaks=K) +
  xlab('Polynomial Degree') +
  ylab('RMSE')
```

Although the training error decreases as we increase the degree, the test error bottoms out at for a fifth degree polynomial.

Let's re-fit this model on all of the data and plot the final result.

```{r}
model <- lm(num_trips ~ poly(tmin, 5, raw = T), data = trips_per_day_train)

trips_per_day_train <- trips_per_day_train %>%
  add_predictions(model) %>%
  mutate(split = "train")
trips_per_day_validate <- trips_per_day_validate %>%
  add_predictions(model) %>%
  mutate(split = "validate")
plot_data <- bind_rows(trips_per_day_train, trips_per_day_validate)

ggplot(plot_data, aes(x = tmin, y = num_trips)) +
  geom_point(aes(color = split)) +
  geom_line(aes(y = pred)) +
  xlab('Minimum temperature') +
  ylab('Daily trips') +
  scale_y_continuous()
```

We're done at this point, with one important exception.

If we'd like to quote how well we expect this model to do on future data, we should use a final, held out test set that we touch only once to make this assessment. (Reusing the validation set would give an optimistic estimate, as our modeling process has already seen that data in the cross-validation process.)



## K_fold cross_validation

The downside to a single train / validation split as done above is that when we don't have tons of data, we could get lucky (or unlucky) in terms of which rows end up in the training and validation sets.

k-fold cross-validation addresses this by first shuffling the data and then partitioning it into k "folds". The train / validation process is repeated, rotating through each fold as the validation data (and the rest as training data). This allows us to get a more stable estimate of generalization error, as well as some idea of uncertainty in that estimate.

First we'll shuffle the data by sampling row numbers without replacement, then we'll assign each row to a fold.

```{r}
set.seed(42)
num_folds <- 5
num_days <- nrow(trips_per_day)

ndx <- sample(1:num_days, num_train, replace=F)

trips_per_day <- trips_per_day[ndx, ] %>%
  mutate(fold = (row_number() %% num_folds) + 1)

head(trips_per_day)
```

Mutate and play around for some features

```{r}
summary(trips_per_day)

trips_per_day$day <- weekdays(as.Date(trips_per_day$ymd))
trips_per_day

trips_per_day$month <- months(as.Date(trips_per_day$ymd))
trips_per_day


```

Now we'll loop through each polynomial degree, as before, but add an inner loop over folds to compute the average validation error

```{r}
# fit a model for each polynomial degree
K <- 1:8
avg_validate_err <- c()
se_validate_err <- c()
for (k in K) {

  # do 5-fold cross-validation within each value of k
  validate_err <- c()
  for (f in 1:num_folds) {
    # fit on the training data
    trips_per_day_train <- filter(trips_per_day, fold != f)
    model <- lm(num_trips ~ poly(tmin, k, raw = T)+I(prcp>0)+day+month, data=trips_per_day_train)

    # evaluate on the validation data
    trips_per_day_validate <- filter(trips_per_day, fold == f)
    validate_err[f] <- sqrt(mean((predict(model, trips_per_day_validate) - trips_per_day_validate$num_trips)^2))
  }

  # compute the average validation error across folds
  # and the standard error on this estimate
  avg_validate_err[k] <- mean(validate_err)
  se_validate_err[k] <- sd(validate_err) / sqrt(num_folds)
}
```

And finally we'll plot the resulting average validation error as a function of polynomial degree.

```{r}
# plot the validate error, highlighting the value of k with the lowest average error
plot_data <- data.frame(K, avg_validate_err, se_validate_err)
ggplot(plot_data, aes(x=K, y=avg_validate_err)) +
  geom_pointrange(aes(ymin=avg_validate_err - se_validate_err,
                      ymax=avg_validate_err + se_validate_err,
                      color=avg_validate_err == min(avg_validate_err))) +
  geom_line(color = "red") +
  scale_x_continuous(breaks=1:12) +
  theme(legend.position="none") +
  xlab('Polynomial Degree') +
  ylab('RMSE on validation data')
```

Here we see that a second degree polynomial is a reasonable choice. This is only because tmin is a numeric variable which can be used in the poly() function. The other three variables are non-numeric, but must have strong enough influence if only a second degree polynomial of tmin produces the lowest RMSE. We are also a bit uncertain as our SE looks about 1,000.


```{r}
summary(model)
```






